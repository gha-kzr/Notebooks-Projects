{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwYVlvhi4-j2"
   },
   "source": [
    "\n",
    "\n",
    "Installation et import des librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XluUC6LZC8KA"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "#import json\n",
    "import math\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6O-yms4vU5R"
   },
   "source": [
    "Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1676147161600,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "vgcH_iaVvRsC"
   },
   "outputs": [],
   "source": [
    "random.seed()\n",
    "\n",
    "# Nombre d'offres d'emploi par page\n",
    "JOB_PER_BATCH = 25 \n",
    "\n",
    "# Temps de repos entre chaque page scrappé pour éviter l'erreur 429 Too Many Request\n",
    "SLEEPING_DELAY = 1\n",
    "\n",
    "# Temps de repos supplémentaire en cas d'erreur 429\n",
    "HARD_SLEEPING_DELAY = 30\n",
    "\n",
    "# Paramètre\n",
    "# Can't scrap more then 1000 offers  (response 400 after start >= 1000)\n",
    "# On va donc découper la requete par ville et utilisant uniquement les dernières 24h si Paris, sinon semaine pour une ville de Province\n",
    "count_url = 'https://fr.linkedin.com/jobs/search?keywords={}&location={}&f_TPR={}&distance=10&position=1&pageNum=0'\n",
    "base_url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords={}&location={}&sortBy=DD&f_TPR={}&distance=10&position=1&pageNum=0&start={}'\n",
    "KEYWORDS = 'data' # Le critère de recherche d'emploi\n",
    "LOCATION = 'Paris' # L'emplacement du job\n",
    "LOCATION_LIST = ['Paris', 'Marseille', 'Lyon', 'Toulouse', 'Nice', 'Nantes', 'Montpellier', 'Strasbourg', 'Bordeaux', 'Lille', 'Rennes', 'Reims', 'Toulon', 'Saint-Étienne', 'Le Havre', 'Grenoble', 'Dijon', 'Angers', 'Nîmes', 'Clermont-Ferrand', 'Aix-en-Provence', 'Le Mans', 'Brest', 'Tours', 'Amiens', 'Limoges', 'Annecy']\n",
    "TPR = 'r86400' # r86400 dernière 24h, r604800 dernière semaine, r2592000 dernier mois\n",
    "sortBy = 'DD' # Sort by DD for date, R for relevance\n",
    "\n",
    "ct_batch = 0\n",
    "ct = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fi89WQQovOBx"
   },
   "source": [
    "Chargement des proxy et headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1676147164276,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "ejNIC98DulG5"
   },
   "outputs": [],
   "source": [
    "# Load proxies\n",
    "df_proxies = pd.read_csv('/workspaces/codespaces-jupyter/data/linkedin_job_analysis/proxies.csv', sep=';')\n",
    "df_proxies = df_proxies.astype({'port': 'str'})\n",
    "df_proxies['url'] = df_proxies['ip'] + \":\" + df_proxies['port']\n",
    "proxies = list('https://' + df_proxies['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1676147166615,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "tFWxMJzpE5ue"
   },
   "outputs": [],
   "source": [
    "headers = [\n",
    "    {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/604.4.7 (KHTML, like Gecko) Version/11.0.2 Safari/604.4.7'},\n",
    "    {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.62 Safari/537.36'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 842,
     "status": "ok",
     "timestamp": 1676147169280,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "jwQfHSmCzChM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sur l'url https://fr.linkedin.com/jobs/search?keywords=data&location=Paris&f_TPR=r86400&distance=10&position=1&pageNum=0\n",
      "Nombre d'offre à récupérer : 85\n"
     ]
    }
   ],
   "source": [
    "proxy = {'http': random.choice(proxies)}\n",
    "header = random.choice(headers)\n",
    "first_url = count_url.format(KEYWORDS, LOCATION, TPR)\n",
    "response = requests.get(first_url, proxies=proxy, headers=header)\n",
    "html = BeautifulSoup(response.text, 'html.parser')\n",
    "try:\n",
    "  job_count = int(html.find('span', {'class': 'results-context-header__job-count'}).text)\n",
    "except Exception as e:\n",
    "  print('Exception : ', e)\n",
    "batch_count = math.ceil(job_count / JOB_PER_BATCH)\n",
    "\n",
    "print(f'Sur l\\'url {first_url}')\n",
    "print(f'Nombre d\\'offre à récupérer : {job_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1676147171319,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "JmVWZSeJ0UwJ"
   },
   "outputs": [],
   "source": [
    "def get_job_batch_html(start):\n",
    "  \"\"\"\n",
    "    Récupère un lot de 25 offres d'emplois sous forme HTML\n",
    "  \"\"\"\n",
    "  global ct_batch\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      time.sleep(SLEEPING_DELAY)\n",
    "      proxy = {'http': random.choice(proxies)}\n",
    "      header = random.choice(headers)\n",
    "      url = base_url.format(KEYWORDS, LOCATION, TPR, start)\n",
    "      #print(proxy, header)\n",
    "      #print(url)\n",
    "      response = requests.get(url, proxies=proxy, headers=header)\n",
    "      #response = requests.get(base_url.format(keywords, location, tpr, start))\n",
    "      if response.status_code == 429:\n",
    "        print(\"Got response status 429\")\n",
    "        time.sleep(HARD_SLEEPING_DELAY)\n",
    "      elif response.status_code == 200:\n",
    "        ct_batch += 1\n",
    "        print(\"Batch done : \" + str(ct_batch))\n",
    "        break\n",
    "      else:\n",
    "        print(\"Nothing with code \"+ str(response.status_code))\n",
    "    except Exception as e:\n",
    "      print('error : ', e)\n",
    "\n",
    "  #print(response.status_code, response.headers)\n",
    "  return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "def get_job_dict(html):\n",
    "  \"\"\"\n",
    "    Extrait les informations des offres d'emploi d'un lot brut HTML d'offres d'emploi\n",
    "  \"\"\"\n",
    "\n",
    "  # On crée un dictionnaire pour stocker les offres d'emplois\n",
    "  # Le format est choisi pour facilement le convertir en dataframe par la suite avec la fonction pd.from_dict()\n",
    "  job_dict = {'id':[], 'title':[], 'company':[], 'url':[], 'location':[], 'date':[], 'description':[]}\n",
    "\n",
    "  # On itère sur chaque offre d'emploi dans le HTML (une carte offre d'emploi est une div avec le paramètre data-entity-urn = True)\n",
    "  # Et on récupère les informations de chaque offre\n",
    "  for job in html.find_all('div', {'data-entity-urn': True}):\n",
    "    id = job['data-entity-urn'].split(\":\")[3]\n",
    "    title = job.find('h3').text.strip()\n",
    "    company = job.find('a', {'class': 'hidden-nested-link'}).text.strip()\n",
    "    job_url = job.find('a', {'class': 'base-card__full-link'})['href']\n",
    "    location = job.find('span', {'class': 'job-search-card__location'}).text.strip()\n",
    "    # date = job.find('time', {'class': 'job-search-card__listdate'})['datetime']\n",
    "\n",
    "    try:   \n",
    "      date = job.find('time')['datetime']\n",
    "    except Exception as e:\n",
    "      date = \"\"\n",
    "      print('Missing date for job :' + job_url)\n",
    "\n",
    "    # On a pas la description de l'offre d'emploi sur cette page, il faudra aller les chercher une par une par la suite\n",
    "    description = \"\"\n",
    "\n",
    "    job_dict['id'].append(id)\n",
    "    job_dict['title'].append(title)\n",
    "    job_dict['company'].append(company)\n",
    "    job_dict['url'].append(job_url)\n",
    "    job_dict['location'].append(location)\n",
    "    job_dict['date'].append(date)\n",
    "    job_dict['description'].append(description)   \n",
    "\n",
    "  return job_dict\n",
    "\n",
    "def get_job_details_html(url): \n",
    "  \"\"\"\n",
    "    Récupère le HTML brut d'une page détail d'une offre d'emploi\n",
    "  \"\"\" \n",
    "  global ct\n",
    "  #sleeping_delay = 1\n",
    "  while True:\n",
    "    try:\n",
    "      time.sleep(SLEEPING_DELAY)\n",
    "      proxy = {'http': random.choice(proxies)}\n",
    "      header = random.choice(headers)\n",
    "      response = requests.get(url, proxies=proxy, headers=header)\n",
    "\n",
    "      if response.status_code == 429:\n",
    "        print(\"Got response status 429\")\n",
    "        time.sleep(HARD_SLEEPING_DELAY)\n",
    "      elif response.status_code == 200:\n",
    "        ct += 1\n",
    "        print(\"Job done : \" + str(ct))\n",
    "        break\n",
    "    except Exception as e:\n",
    "      print('Exception : ', e)\n",
    "\n",
    "  return BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "def get_job_description(html):\n",
    "  \"\"\"\n",
    "    Extrait la description du HTML brut d'une page d'emploi\n",
    "  \"\"\"\n",
    "  description = html.find('div', {'class': 'show-more-less-html__markup'})\n",
    "  try:\n",
    "    description = description.get_text(separator=\"\\n\")\n",
    "  except AttributeError:\n",
    "      print(\"No description\")\n",
    "  return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1676147174472,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "6cnakFPZCT5R"
   },
   "outputs": [],
   "source": [
    "def retrieve_n_batch(ct_batch):  \n",
    "  job_dict = {'id':[], 'title':[],'company':[],'url':[],'location':[],'date':[],'description':[]}\n",
    "\n",
    "  for index in range(ct_batch):\n",
    "    start_position = index * JOB_PER_BATCH\n",
    "    html = get_job_batch_html(start_position)\n",
    "    job_dict_tmp = get_job_dict(html)\n",
    "\n",
    "    job_dict['id'] += job_dict_tmp['id']\n",
    "    job_dict['title'] += job_dict_tmp['title']\n",
    "    job_dict['company'] += job_dict_tmp['company']\n",
    "    job_dict['url'] += job_dict_tmp['url']\n",
    "    job_dict['location'] += job_dict_tmp['location']\n",
    "    job_dict['date'] += job_dict_tmp['date']\n",
    "    job_dict['description'] += job_dict_tmp['description']\n",
    "\n",
    "  return job_dict\n",
    "\n",
    "def retrieve_description(job_dict):\n",
    "  for idx, url in enumerate(job_dict['url']):\n",
    "    job_html = get_job_details_html(url)\n",
    "    job_description = get_job_description(job_html)\n",
    "    job_dict['description'][idx] = job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1883,
     "status": "ok",
     "timestamp": 1676147178350,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "Z6tWMjJqQADT",
    "outputId": "395e8b41-3cb8-4dad-c27a-5d2298b91b87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch done : 1\n",
      "Batch done : 2\n",
      "Batch done : 3\n"
     ]
    }
   ],
   "source": [
    "jobs = retrieve_n_batch(batch_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1676147181180,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "215IiBV2Tmum"
   },
   "outputs": [],
   "source": [
    "#with open(\"/workspaces/codespaces-jupyter/data/linkedin_job_analysis/part_1_job_list.json\", \"w\") as outfile:\n",
    "#    json.dump(jobs, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36266,
     "status": "ok",
     "timestamp": 1676147218400,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "SlLszTCgTh4z",
    "outputId": "b2399a20-307d-46f6-a20e-9e320be68dd7"
   },
   "outputs": [],
   "source": [
    "retrieve_description(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1676147225593,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "JB1pYPDJArYk",
    "outputId": "21dc7fc0-5f38-4018-d64c-05870767d6be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3520314506</td>\n",
       "      <td>Analyste de données Service Clients (F/H)</td>\n",
       "      <td>Louis Vuitton</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/analyste-de-...</td>\n",
       "      <td>Paris, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n\\nAbout The Job\\nDepuis plus de 150 ans, les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3516932318</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Jobs via eFinancialCareers</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>Paris, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n        Vos responsabilités comprendront not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3524834557</td>\n",
       "      <td>Data Analyst / Data Scientist F/H en Alternance</td>\n",
       "      <td>Carrefour</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>Massy, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n\\nA propos de nous:\\nCréateur de l’hypermarc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3524734366</td>\n",
       "      <td>Pmo Ou Chef De Projet Transformation Digitale ...</td>\n",
       "      <td>Merck Génériques</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/pmo-ou-chef-...</td>\n",
       "      <td>Sèvres, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n        Dans un souci d’accessibilité et de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3373273124</td>\n",
       "      <td>Data Scientist H/F</td>\n",
       "      <td>Capgemini Engineering</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-scienti...</td>\n",
       "      <td>Vélizy-Villacoublay, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n        Notre offre\\nTESSELLA est le World C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  3520314506          Analyste de données Service Clients (F/H)   \n",
       "1  3516932318                                       Data Analyst   \n",
       "2  3524834557    Data Analyst / Data Scientist F/H en Alternance   \n",
       "3  3524734366  Pmo Ou Chef De Projet Transformation Digitale ...   \n",
       "4  3373273124                                 Data Scientist H/F   \n",
       "\n",
       "                      company  \\\n",
       "0               Louis Vuitton   \n",
       "1  Jobs via eFinancialCareers   \n",
       "2                   Carrefour   \n",
       "3            Merck Génériques   \n",
       "4       Capgemini Engineering   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://fr.linkedin.com/jobs/view/analyste-de-...   \n",
       "1  https://fr.linkedin.com/jobs/view/data-analyst...   \n",
       "2  https://fr.linkedin.com/jobs/view/data-analyst...   \n",
       "3  https://fr.linkedin.com/jobs/view/pmo-ou-chef-...   \n",
       "4  https://fr.linkedin.com/jobs/view/data-scienti...   \n",
       "\n",
       "                                     location        date  \\\n",
       "0                Paris, Île-de-France, France  2023-03-12   \n",
       "1                Paris, Île-de-France, France  2023-03-12   \n",
       "2                Massy, Île-de-France, France  2023-03-12   \n",
       "3               Sèvres, Île-de-France, France  2023-03-12   \n",
       "4  Vélizy-Villacoublay, Île-de-France, France  2023-03-12   \n",
       "\n",
       "                                         description  \n",
       "0  \\n\\nAbout The Job\\nDepuis plus de 150 ans, les...  \n",
       "1  \\n        Vos responsabilités comprendront not...  \n",
       "2  \\n\\nA propos de nous:\\nCréateur de l’hypermarc...  \n",
       "3  \\n        Dans un souci d’accessibilité et de ...  \n",
       "4  \\n        Notre offre\\nTESSELLA est le World C...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_job = pd.DataFrame.from_dict(jobs)\n",
    "df_job.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1676147232399,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "Jq9I-vrwA0ha"
   },
   "outputs": [],
   "source": [
    "df_job.to_csv('/workspaces/codespaces-jupyter/data/linkedin_job_analysis/part_1_job_list.csv', sep=',', encoding='utf-16', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1676147234002,
     "user": {
      "displayName": "Guillaume",
      "userId": "06398127721149676426"
     },
     "user_tz": -60
    },
    "id": "uGoFF87xFiD5",
    "outputId": "1e1f58ac-ec7f-4ada-c6d9-c955db0c69f8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>url</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3520314506</td>\n",
       "      <td>Analyste de données Service Clients (F/H)</td>\n",
       "      <td>Louis Vuitton</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/analyste-de-...</td>\n",
       "      <td>Paris, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n\\nAbout The Job\\nDepuis plus de 150 ans, les...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3516932318</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Jobs via eFinancialCareers</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>Paris, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n        Vos responsabilités comprendront not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3524834557</td>\n",
       "      <td>Data Analyst / Data Scientist F/H en Alternance</td>\n",
       "      <td>Carrefour</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-analyst...</td>\n",
       "      <td>Massy, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n\\nA propos de nous:\\nCréateur de l’hypermarc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3524734366</td>\n",
       "      <td>Pmo Ou Chef De Projet Transformation Digitale ...</td>\n",
       "      <td>Merck Génériques</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/pmo-ou-chef-...</td>\n",
       "      <td>Sèvres, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n        Dans un souci d’accessibilité et de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3373273124</td>\n",
       "      <td>Data Scientist H/F</td>\n",
       "      <td>Capgemini Engineering</td>\n",
       "      <td>https://fr.linkedin.com/jobs/view/data-scienti...</td>\n",
       "      <td>Vélizy-Villacoublay, Île-de-France, France</td>\n",
       "      <td>2023-03-12</td>\n",
       "      <td>\\n        Notre offre\\nTESSELLA est le World C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title  \\\n",
       "0  3520314506          Analyste de données Service Clients (F/H)   \n",
       "1  3516932318                                       Data Analyst   \n",
       "2  3524834557    Data Analyst / Data Scientist F/H en Alternance   \n",
       "3  3524734366  Pmo Ou Chef De Projet Transformation Digitale ...   \n",
       "4  3373273124                                 Data Scientist H/F   \n",
       "\n",
       "                      company  \\\n",
       "0               Louis Vuitton   \n",
       "1  Jobs via eFinancialCareers   \n",
       "2                   Carrefour   \n",
       "3            Merck Génériques   \n",
       "4       Capgemini Engineering   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://fr.linkedin.com/jobs/view/analyste-de-...   \n",
       "1  https://fr.linkedin.com/jobs/view/data-analyst...   \n",
       "2  https://fr.linkedin.com/jobs/view/data-analyst...   \n",
       "3  https://fr.linkedin.com/jobs/view/pmo-ou-chef-...   \n",
       "4  https://fr.linkedin.com/jobs/view/data-scienti...   \n",
       "\n",
       "                                     location        date  \\\n",
       "0                Paris, Île-de-France, France  2023-03-12   \n",
       "1                Paris, Île-de-France, France  2023-03-12   \n",
       "2                Massy, Île-de-France, France  2023-03-12   \n",
       "3               Sèvres, Île-de-France, France  2023-03-12   \n",
       "4  Vélizy-Villacoublay, Île-de-France, France  2023-03-12   \n",
       "\n",
       "                                         description  \n",
       "0  \\n\\nAbout The Job\\nDepuis plus de 150 ans, les...  \n",
       "1  \\n        Vos responsabilités comprendront not...  \n",
       "2  \\n\\nA propos de nous:\\nCréateur de l’hypermarc...  \n",
       "3  \\n        Dans un souci d’accessibilité et de ...  \n",
       "4  \\n        Notre offre\\nTESSELLA est le World C...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('/workspaces/codespaces-jupyter/data/linkedin_job_analysis/part_1_job_list.csv', sep=',', encoding='utf-16')\n",
    "df_test.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO7nc3HXYlDfEyBaRJbJA+3",
   "mount_file_id": "1h25H2srDiSOAkK2Cgb3djz7mJ2EI4aTT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
